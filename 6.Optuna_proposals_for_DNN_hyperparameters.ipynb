{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34aa8846-965d-4898-84f0-66d8c7e4384e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df:  (19501, 1934)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-05 12:24:54,602] A new study created in memory with name: no-name-c24365f7-5174-473d-abc0-ae16f4813cc4\n",
      "[I 2025-04-05 12:24:56,748] Trial 0 finished with value: 0.80390625 and parameters: {'n_layers': 2, 'n_units_l0': 26, 'dropout_l0': 0.34984199764831103, 'n_units_l1': 90, 'dropout_l1': 0.4603260984935838, 'optimizer': 'RMSprop', 'lr': 0.0012925896712629964}. Best is trial 0 with value: 0.80390625.\n",
      "[I 2025-04-05 12:24:59,336] Trial 1 finished with value: 0.59609375 and parameters: {'n_layers': 3, 'n_units_l0': 69, 'dropout_l0': 0.401403467302053, 'n_units_l1': 113, 'dropout_l1': 0.3027474353632066, 'n_units_l2': 101, 'dropout_l2': 0.2565112658040606, 'optimizer': 'Adam', 'lr': 4.4919104848397124e-05}. Best is trial 0 with value: 0.80390625.\n",
      "[I 2025-04-05 12:25:01,067] Trial 2 finished with value: 0.790625 and parameters: {'n_layers': 1, 'n_units_l0': 43, 'dropout_l0': 0.2022025221362281, 'optimizer': 'RMSprop', 'lr': 0.004503170457003042}. Best is trial 0 with value: 0.80390625.\n",
      "[I 2025-04-05 12:25:02,857] Trial 3 finished with value: 0.41171875 and parameters: {'n_layers': 2, 'n_units_l0': 122, 'dropout_l0': 0.2540030827315592, 'n_units_l1': 50, 'dropout_l1': 0.3299084212045851, 'optimizer': 'SGD', 'lr': 0.0004678201769150414}. Best is trial 0 with value: 0.80390625.\n",
      "[I 2025-04-05 12:25:05,247] Trial 4 finished with value: 0.59609375 and parameters: {'n_layers': 3, 'n_units_l0': 47, 'dropout_l0': 0.36502772151446894, 'n_units_l1': 31, 'dropout_l1': 0.45713745970026287, 'n_units_l2': 73, 'dropout_l2': 0.35517462716543075, 'optimizer': 'Adam', 'lr': 2.7986506799305192e-05}. Best is trial 0 with value: 0.80390625.\n",
      "[I 2025-04-05 12:25:06,856] Trial 5 finished with value: 0.765625 and parameters: {'n_layers': 1, 'n_units_l0': 123, 'dropout_l0': 0.2301069749708969, 'optimizer': 'SGD', 'lr': 0.01161554344034101}. Best is trial 0 with value: 0.80390625.\n",
      "[I 2025-04-05 12:25:07,095] Trial 6 pruned. \n",
      "[I 2025-04-05 12:25:07,296] Trial 7 pruned. \n",
      "[I 2025-04-05 12:25:07,513] Trial 8 pruned. \n",
      "[I 2025-04-05 12:25:07,687] Trial 9 pruned. \n",
      "[I 2025-04-05 12:25:07,865] Trial 10 pruned. \n",
      "[I 2025-04-05 12:25:09,543] Trial 11 finished with value: 0.7875 and parameters: {'n_layers': 1, 'n_units_l0': 20, 'dropout_l0': 0.42043461655665565, 'optimizer': 'RMSprop', 'lr': 0.000511987822049892}. Best is trial 0 with value: 0.80390625.\n",
      "[I 2025-04-05 12:25:11,292] Trial 12 finished with value: 0.80078125 and parameters: {'n_layers': 1, 'n_units_l0': 31, 'dropout_l0': 0.27346513971693376, 'optimizer': 'RMSprop', 'lr': 0.002179034993721914}. Best is trial 0 with value: 0.80390625.\n",
      "[I 2025-04-05 12:25:13,292] Trial 13 finished with value: 0.80703125 and parameters: {'n_layers': 2, 'n_units_l0': 28, 'dropout_l0': 0.27785836154203514, 'n_units_l1': 126, 'dropout_l1': 0.39856244155720366, 'optimizer': 'RMSprop', 'lr': 0.0006317303061901066}. Best is trial 13 with value: 0.80703125.\n",
      "[I 2025-04-05 12:25:15,408] Trial 14 finished with value: 0.81328125 and parameters: {'n_layers': 2, 'n_units_l0': 88, 'dropout_l0': 0.35733447790184836, 'n_units_l1': 127, 'dropout_l1': 0.3990879216341347, 'optimizer': 'RMSprop', 'lr': 0.00022437792749034345}. Best is trial 14 with value: 0.81328125.\n",
      "[I 2025-04-05 12:25:17,497] Trial 15 finished with value: 0.81171875 and parameters: {'n_layers': 2, 'n_units_l0': 92, 'dropout_l0': 0.2903325486270838, 'n_units_l1': 127, 'dropout_l1': 0.39812664551169297, 'optimizer': 'RMSprop', 'lr': 0.00023137422400385668}. Best is trial 14 with value: 0.81328125.\n",
      "[I 2025-04-05 12:25:17,722] Trial 16 pruned. \n",
      "[I 2025-04-05 12:25:17,966] Trial 17 pruned. \n",
      "[I 2025-04-05 12:25:18,192] Trial 18 pruned. \n",
      "[I 2025-04-05 12:25:18,457] Trial 19 pruned. \n",
      "[I 2025-04-05 12:25:18,658] Trial 20 pruned. \n",
      "[I 2025-04-05 12:25:20,746] Trial 21 finished with value: 0.8078125 and parameters: {'n_layers': 2, 'n_units_l0': 76, 'dropout_l0': 0.2664176225394222, 'n_units_l1': 128, 'dropout_l1': 0.4009728792118756, 'optimizer': 'RMSprop', 'lr': 0.0006026950299998549}. Best is trial 14 with value: 0.81328125.\n",
      "[I 2025-04-05 12:25:21,210] Trial 22 pruned. \n",
      "[I 2025-04-05 12:25:23,303] Trial 23 finished with value: 0.81640625 and parameters: {'n_layers': 2, 'n_units_l0': 79, 'dropout_l0': 0.34285273367145525, 'n_units_l1': 102, 'dropout_l1': 0.37262807266911147, 'optimizer': 'RMSprop', 'lr': 0.0009399059872344642}. Best is trial 23 with value: 0.81640625.\n",
      "[I 2025-04-05 12:25:25,447] Trial 24 finished with value: 0.8234375 and parameters: {'n_layers': 2, 'n_units_l0': 113, 'dropout_l0': 0.3316500162114466, 'n_units_l1': 104, 'dropout_l1': 0.3691858686507848, 'optimizer': 'RMSprop', 'lr': 0.0013291533636181113}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:25:25,685] Trial 25 pruned. \n",
      "[I 2025-04-05 12:25:27,574] Trial 26 finished with value: 0.80703125 and parameters: {'n_layers': 1, 'n_units_l0': 113, 'dropout_l0': 0.38054833990552905, 'optimizer': 'RMSprop', 'lr': 0.0012818953206102407}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:25:27,998] Trial 27 pruned. \n",
      "[I 2025-04-05 12:25:30,314] Trial 28 finished with value: 0.80625 and parameters: {'n_layers': 2, 'n_units_l0': 116, 'dropout_l0': 0.3782981424078968, 'n_units_l1': 106, 'dropout_l1': 0.3101878811934149, 'optimizer': 'Adam', 'lr': 0.0011048355642621192}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:25:30,480] Trial 29 pruned. \n",
      "[I 2025-04-05 12:25:30,746] Trial 30 pruned. \n",
      "[I 2025-04-05 12:25:30,983] Trial 31 pruned. \n",
      "[I 2025-04-05 12:25:31,220] Trial 32 pruned. \n",
      "[I 2025-04-05 12:25:33,349] Trial 33 finished with value: 0.815625 and parameters: {'n_layers': 2, 'n_units_l0': 95, 'dropout_l0': 0.3120065542742672, 'n_units_l1': 119, 'dropout_l1': 0.39133933768834106, 'optimizer': 'RMSprop', 'lr': 0.0007787860878392601}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:25:35,455] Trial 34 finished with value: 0.8078125 and parameters: {'n_layers': 2, 'n_units_l0': 83, 'dropout_l0': 0.313285880961261, 'n_units_l1': 115, 'dropout_l1': 0.381299518437889, 'optimizer': 'RMSprop', 'lr': 0.0009011539761932354}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:25:35,698] Trial 35 pruned. \n",
      "[I 2025-04-05 12:25:35,967] Trial 36 pruned. \n",
      "[I 2025-04-05 12:25:36,201] Trial 37 pruned. \n",
      "[I 2025-04-05 12:25:36,437] Trial 38 pruned. \n",
      "[I 2025-04-05 12:25:36,696] Trial 39 pruned. \n",
      "[I 2025-04-05 12:25:38,756] Trial 40 finished with value: 0.80859375 and parameters: {'n_layers': 2, 'n_units_l0': 76, 'dropout_l0': 0.4240291735882029, 'n_units_l1': 60, 'dropout_l1': 0.27911851064091164, 'optimizer': 'RMSprop', 'lr': 0.0017759560271379889}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:25:38,997] Trial 41 pruned. \n",
      "[I 2025-04-05 12:25:39,247] Trial 42 pruned. \n",
      "[I 2025-04-05 12:25:41,368] Trial 43 finished with value: 0.81640625 and parameters: {'n_layers': 2, 'n_units_l0': 86, 'dropout_l0': 0.3131794206368873, 'n_units_l1': 121, 'dropout_l1': 0.3937332538561388, 'optimizer': 'RMSprop', 'lr': 0.0004703636195404253}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:25:41,574] Trial 44 pruned. \n",
      "[I 2025-04-05 12:25:43,932] Trial 45 finished with value: 0.81328125 and parameters: {'n_layers': 3, 'n_units_l0': 85, 'dropout_l0': 0.32435978979816654, 'n_units_l1': 100, 'dropout_l1': 0.34544005330187255, 'n_units_l2': 79, 'dropout_l2': 0.3075106030705169, 'optimizer': 'RMSprop', 'lr': 0.0008709789019756462}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:25:44,112] Trial 46 pruned. \n",
      "[I 2025-04-05 12:25:44,746] Trial 47 pruned. \n",
      "[I 2025-04-05 12:25:45,834] Trial 48 pruned. \n",
      "[I 2025-04-05 12:25:46,048] Trial 49 pruned. \n",
      "[I 2025-04-05 12:25:46,317] Trial 50 pruned. \n",
      "[I 2025-04-05 12:25:48,728] Trial 51 finished with value: 0.80546875 and parameters: {'n_layers': 3, 'n_units_l0': 87, 'dropout_l0': 0.32967505839227945, 'n_units_l1': 97, 'dropout_l1': 0.3461183497134263, 'n_units_l2': 91, 'dropout_l2': 0.2970892959964389, 'optimizer': 'RMSprop', 'lr': 0.0007133511349040143}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:25:51,167] Trial 52 finished with value: 0.80625 and parameters: {'n_layers': 3, 'n_units_l0': 85, 'dropout_l0': 0.31977921866853837, 'n_units_l1': 105, 'dropout_l1': 0.32322567731597523, 'n_units_l2': 125, 'dropout_l2': 0.31460932600065306, 'optimizer': 'RMSprop', 'lr': 0.0016051488724471543}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:25:51,399] Trial 53 pruned. \n",
      "[I 2025-04-05 12:25:51,638] Trial 54 pruned. \n",
      "[I 2025-04-05 12:25:51,839] Trial 55 pruned. \n",
      "[I 2025-04-05 12:25:52,101] Trial 56 pruned. \n",
      "[I 2025-04-05 12:25:52,383] Trial 57 pruned. \n",
      "[I 2025-04-05 12:25:52,622] Trial 58 pruned. \n",
      "[I 2025-04-05 12:25:53,444] Trial 59 pruned. \n",
      "[I 2025-04-05 12:25:53,643] Trial 60 pruned. \n",
      "[I 2025-04-05 12:25:53,902] Trial 61 pruned. \n",
      "[I 2025-04-05 12:25:54,345] Trial 62 pruned. \n",
      "[I 2025-04-05 12:25:54,593] Trial 63 pruned. \n",
      "[I 2025-04-05 12:25:55,263] Trial 64 pruned. \n",
      "[I 2025-04-05 12:25:56,558] Trial 65 pruned. \n",
      "[I 2025-04-05 12:25:57,012] Trial 66 pruned. \n",
      "[I 2025-04-05 12:25:57,230] Trial 67 pruned. \n",
      "[I 2025-04-05 12:25:57,489] Trial 68 pruned. \n",
      "[I 2025-04-05 12:25:57,755] Trial 69 pruned. \n",
      "[I 2025-04-05 12:25:58,198] Trial 70 pruned. \n",
      "[I 2025-04-05 12:26:00,294] Trial 71 finished with value: 0.8140625 and parameters: {'n_layers': 2, 'n_units_l0': 90, 'dropout_l0': 0.4197277324933272, 'n_units_l1': 61, 'dropout_l1': 0.2791105390192805, 'optimizer': 'RMSprop', 'lr': 0.0017747983734728143}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:26:00,533] Trial 72 pruned. \n",
      "[I 2025-04-05 12:26:00,776] Trial 73 pruned. \n",
      "[I 2025-04-05 12:26:01,015] Trial 74 pruned. \n",
      "[I 2025-04-05 12:26:03,125] Trial 75 finished with value: 0.81015625 and parameters: {'n_layers': 2, 'n_units_l0': 111, 'dropout_l0': 0.2778404928978003, 'n_units_l1': 55, 'dropout_l1': 0.2345765166681028, 'optimizer': 'RMSprop', 'lr': 0.0009518928856231456}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:26:03,568] Trial 76 pruned. \n",
      "[I 2025-04-05 12:26:05,704] Trial 77 finished with value: 0.8109375 and parameters: {'n_layers': 2, 'n_units_l0': 93, 'dropout_l0': 0.416174169247162, 'n_units_l1': 86, 'dropout_l1': 0.4309124330914858, 'optimizer': 'RMSprop', 'lr': 0.0006540158836913475}. Best is trial 24 with value: 0.8234375.\n",
      "[I 2025-04-05 12:26:05,939] Trial 78 pruned. \n",
      "[I 2025-04-05 12:26:06,170] Trial 79 pruned. \n",
      "[I 2025-04-05 12:26:06,482] Trial 80 pruned. \n",
      "[I 2025-04-05 12:26:06,718] Trial 81 pruned. \n",
      "[I 2025-04-05 12:26:07,186] Trial 82 pruned. \n",
      "[I 2025-04-05 12:26:07,429] Trial 83 pruned. \n",
      "[I 2025-04-05 12:26:07,878] Trial 84 pruned. \n",
      "[I 2025-04-05 12:26:08,113] Trial 85 pruned. \n",
      "[I 2025-04-05 12:26:08,359] Trial 86 pruned. \n",
      "[I 2025-04-05 12:26:08,609] Trial 87 pruned. \n",
      "[I 2025-04-05 12:26:09,052] Trial 88 pruned. \n",
      "[I 2025-04-05 12:26:09,285] Trial 89 pruned. \n",
      "[I 2025-04-05 12:26:09,539] Trial 90 pruned. \n",
      "[I 2025-04-05 12:26:10,014] Trial 91 pruned. \n",
      "[I 2025-04-05 12:26:10,474] Trial 92 pruned. \n",
      "[I 2025-04-05 12:26:10,927] Trial 93 pruned. \n",
      "[I 2025-04-05 12:26:11,172] Trial 94 pruned. \n",
      "[I 2025-04-05 12:26:11,418] Trial 95 pruned. \n",
      "[I 2025-04-05 12:26:11,872] Trial 96 pruned. \n",
      "[I 2025-04-05 12:26:12,118] Trial 97 pruned. \n",
      "[I 2025-04-05 12:26:12,344] Trial 98 pruned. \n",
      "[I 2025-04-05 12:26:12,605] Trial 99 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  74\n",
      "  Number of complete trials:  26\n",
      "Best trial:\n",
      "  Value:  0.8234375\n",
      "  Params: \n",
      "    n_layers: 2\n",
      "    n_units_l0: 113\n",
      "    dropout_l0: 0.3316500162114466\n",
      "    n_units_l1: 104\n",
      "    dropout_l1: 0.3691858686507848\n",
      "    optimizer: RMSprop\n",
      "    lr: 0.0013291533636181113\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "# from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "BATCHSIZE = 128\n",
    "CLASSES = 2\n",
    "DIR = os.getcwd()\n",
    "EPOCHS = 10\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 10\n",
    "\n",
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 1933\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Load your DataFrame (replace 'your_data.csv' with your actual file)\n",
    "df = pd.read_csv(\"dataset2_and_atomic_features.csv\", index_col=0)\n",
    "\n",
    "# Display the data frame\n",
    "print('Shape of df: ', df.shape)\n",
    "df.head()\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = df.iloc[:, :-1].values.astype(np.float32)  # Features\n",
    "y = df.iloc[:, -1].values.astype(np.float32)  # Target\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the *training* data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the training data\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Transform the *testing* data using the same scaler\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "y_train_tensor = torch.tensor(y_train).long()\n",
    "X_val_tensor = torch.tensor(X_val)\n",
    "y_val_tensor = torch.tensor(y_val).long()\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCHSIZE, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=BATCHSIZE, shuffle=False)\n",
    "\n",
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    # Get the FashionMNIST dataset.\n",
    "    # train_loader, valid_loader = get_mnist()\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Limiting training data for faster epochs.\n",
    "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "                break\n",
    "\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                # Limiting validation data.\n",
    "                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                    break\n",
    "                # data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
    "                output = model(data)\n",
    "                # Get the index of the max log-probability.\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
